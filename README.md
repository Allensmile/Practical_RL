# Practical_RL
A course on reinforcement learning in the wild.

# Coordinates
* CS dept @ HSE. Classes are on mondays at 18-10 in Room 503 (or 505 or 625, lemme check next week).
* E-mail for submitting homeworks and stuff: __practicalrl17@gmail.com__
* Magic button that creates VM: [![Binder](http://mybinder.org/badge.svg)](http://mybinder.org:/repo/yandexdataschool/practical_rl)
* Course chat room: [![Gitter](https://camo.githubusercontent.com/90098c3e02322a58e3f4eba3837b3d3127e7da4f/68747470733a2f2f6261646765732e6769747465722e696d2f79616e646578646174617363686f6f6c2f4167656e744e65742e737667)](https://gitter.im/yandexdataschool/Practical_RL)

# Announcements
* 7.02.17 - HWs checked up
* 6.02.17 - week2 uploaded
* 27.01.17 - merged fix by _omtcyfz_, thanks!
* 27.01.17 - added course mail for homework submission: __practicalrl17@gmail.com__
* 23.01.17 - first class happened
* 23.01.17 - created repo

# Syllabus
* __week0__ Welcome to the MDP
 * Lecture: RL problems around us. Markov decision process. Simple solutions through combinatoric optimization.
 * Seminar: Frozenlake with genetic algorithms
    * HSE Homework deadline: _23.59 1.02.17_
* __week1__ Monte-carlo methods
 * Lecture: Crossentropy method in general and for RL. Extension to continuous state & action space. Limitations.
 * Seminar: Tabular CEM for Taxi-v0, deep CEM for box2d environments.
    * HSE homework deadline: _23.59 8.02.17_
* __week2__ Temporal Difference
 * Lecture: Discounted reward MDP. Value iteration. Q-learning. Temporal difference Vs Monte-Carlo.
 * Seminar: Tabular q-learning
    * You have to do either ./week2/assignment (py2 only) or ./week2/alternative (any python)
    * HSE homework deadline: _23.59 8.02.17_

* __week3__ Value-based algorithms
 * Lecture: SARSA. Off-policy Vs on-policy algorithms. N-step algorithms. Eligibility traces.
 * Seminar: Qlearning Vs SARSA Vs expected value sarsa in the wild
 
