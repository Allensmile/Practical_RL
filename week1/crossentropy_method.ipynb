{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossentropy method\n",
    "\n",
    "This notebook will teach you to solve reinforcement learning with crossentropy method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-29 15:43:14,909] Making new env: Taxi-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[43mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_states=500, n_actions=6\n"
     ]
    }
   ],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"n_states=%i, n_actions=%i\"%(n_states,n_actions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create stochastic policy\n",
    "\n",
    "This time our policy should be a probability distribution.\n",
    "\n",
    "```policy[s,a] = P(take action a | in state s)```\n",
    "\n",
    "Since we still use integer state and action representations, you can use a 2-dimensional array to represent the policy.\n",
    "\n",
    "Please initialize policy __uniformly__, that is, probabililities of all actions should be equal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy = <your code here! Create an array to store action probabilities>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert type(policy) in (np.ndarray,np.matrix)\n",
    "assert np.allclose(policy,1./n_actions)\n",
    "assert np.allclose(np.sum(policy,axis=1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play the game\n",
    "\n",
    "Just like before, but we also record all states and actions we took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=10**4):\n",
    "    \"\"\"\n",
    "    Play game until end or for t_max ticks.\n",
    "    returns: list of states, list of actions and sum of rewards\n",
    "    \"\"\"\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0.\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        a = <pick action from policy (at random with probabilities)>\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        <record prev state, action and add up reward to states,actions and total_reward accordingly>\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s,a,r = generate_session()\n",
    "assert type(s) == type(a) == list\n",
    "assert len(s) == len(a)\n",
    "assert type(r) is float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Generate sessions, select N best and fit to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.4 s, sys: 76 ms, total: 51.4 s\n",
      "Wall time: 52.9 s\n",
      "mean reward = -7925.98800\tthreshold = -5174.5\n",
      "CPU times: user 13.6 s, sys: 28 ms, total: 13.6 s\n",
      "Wall time: 14 s\n",
      "mean reward = -2028.33200\tthreshold = -1316.5\n",
      "CPU times: user 4.98 s, sys: 12 ms, total: 4.99 s\n",
      "Wall time: 5.13 s\n",
      "mean reward = -712.89200\tthreshold = -581.5\n",
      "CPU times: user 2.49 s, sys: 4 ms, total: 2.49 s\n",
      "Wall time: 2.52 s\n",
      "mean reward = -332.32800\tthreshold = -289.5\n",
      "CPU times: user 1.51 s, sys: 12 ms, total: 1.52 s\n",
      "Wall time: 1.56 s\n",
      "mean reward = -175.61600\tthreshold = -153.5\n",
      "CPU times: user 1.08 s, sys: 0 ns, total: 1.08 s\n",
      "Wall time: 1.12 s\n",
      "mean reward = -108.96800\tthreshold = -90.5\n",
      "CPU times: user 1.12 s, sys: 0 ns, total: 1.12 s\n",
      "Wall time: 1.14 s\n",
      "mean reward = -103.50000\tthreshold = -59.0\n",
      "CPU times: user 868 ms, sys: 8 ms, total: 876 ms\n",
      "Wall time: 885 ms\n",
      "mean reward = -66.30000\tthreshold = -41.0\n",
      "CPU times: user 952 ms, sys: 0 ns, total: 952 ms\n",
      "Wall time: 981 ms\n",
      "mean reward = -82.69200\tthreshold = -32.5\n",
      "CPU times: user 852 ms, sys: 0 ns, total: 852 ms\n",
      "Wall time: 870 ms\n",
      "mean reward = -60.44000\tthreshold = -26.0\n",
      "CPU times: user 1.04 s, sys: 4 ms, total: 1.05 s\n",
      "Wall time: 1.08 s\n",
      "mean reward = -90.23600\tthreshold = -29.5\n",
      "CPU times: user 792 ms, sys: 0 ns, total: 792 ms\n",
      "Wall time: 795 ms\n",
      "mean reward = -51.63200\tthreshold = -21.0\n",
      "CPU times: user 752 ms, sys: 0 ns, total: 752 ms\n",
      "Wall time: 761 ms\n",
      "mean reward = -46.29600\tthreshold = -11.0\n",
      "CPU times: user 932 ms, sys: 0 ns, total: 932 ms\n",
      "Wall time: 958 ms\n",
      "mean reward = -75.78800\tthreshold = -16.0\n",
      "CPU times: user 932 ms, sys: 8 ms, total: 940 ms\n",
      "Wall time: 954 ms\n",
      "mean reward = -75.36400\tthreshold = -13.5\n",
      "CPU times: user 824 ms, sys: 0 ns, total: 824 ms\n",
      "Wall time: 833 ms\n",
      "mean reward = -59.43600\tthreshold = -10.0\n",
      "CPU times: user 844 ms, sys: 0 ns, total: 844 ms\n",
      "Wall time: 854 ms\n",
      "mean reward = -60.39200\tthreshold = -11.0\n",
      "CPU times: user 792 ms, sys: 0 ns, total: 792 ms\n",
      "Wall time: 807 ms\n",
      "mean reward = -52.63200\tthreshold = -12.5\n",
      "CPU times: user 776 ms, sys: 0 ns, total: 776 ms\n",
      "Wall time: 782 ms\n",
      "mean reward = -53.76400\tthreshold = -10.5\n",
      "CPU times: user 988 ms, sys: 0 ns, total: 988 ms\n",
      "Wall time: 998 ms\n",
      "mean reward = -88.26000\tthreshold = -12.0\n",
      "CPU times: user 868 ms, sys: 4 ms, total: 872 ms\n",
      "Wall time: 912 ms\n",
      "mean reward = -70.60400\tthreshold = -12.0\n",
      "CPU times: user 1.02 s, sys: 0 ns, total: 1.02 s\n",
      "Wall time: 1.02 s\n",
      "mean reward = -92.02000\tthreshold = -16.5\n",
      "CPU times: user 816 ms, sys: 4 ms, total: 820 ms\n",
      "Wall time: 825 ms\n",
      "mean reward = -57.66400\tthreshold = -8.5\n",
      "CPU times: user 2.16 s, sys: 8 ms, total: 2.17 s\n",
      "Wall time: 2.24 s\n",
      "mean reward = -267.52800\tthreshold = -11.0\n",
      "CPU times: user 3.7 s, sys: 4 ms, total: 3.7 s\n",
      "Wall time: 3.77 s\n",
      "mean reward = -496.59200\tthreshold = -13.0\n",
      "CPU times: user 2.65 s, sys: 0 ns, total: 2.65 s\n",
      "Wall time: 2.8 s\n",
      "mean reward = -348.92800\tthreshold = -12.5\n",
      "CPU times: user 3 s, sys: 12 ms, total: 3.01 s\n",
      "Wall time: 3.1 s\n",
      "mean reward = -402.50800\tthreshold = -13.5\n",
      "CPU times: user 3.5 s, sys: 12 ms, total: 3.51 s\n",
      "Wall time: 3.77 s\n",
      "mean reward = -442.25200\tthreshold = -10.5\n",
      "CPU times: user 2.86 s, sys: 0 ns, total: 2.86 s\n",
      "Wall time: 2.98 s\n",
      "mean reward = -352.83600\tthreshold = -9.5\n",
      "CPU times: user 2.38 s, sys: 12 ms, total: 2.39 s\n",
      "Wall time: 2.6 s\n",
      "mean reward = -291.73600\tthreshold = -10.5\n",
      "CPU times: user 3.26 s, sys: 8 ms, total: 3.26 s\n",
      "Wall time: 3.39 s\n",
      "mean reward = -432.42000\tthreshold = -15.0\n",
      "CPU times: user 4.36 s, sys: 8 ms, total: 4.37 s\n",
      "Wall time: 4.42 s\n",
      "mean reward = -628.35600\tthreshold = -24.5\n",
      "CPU times: user 4.68 s, sys: 8 ms, total: 4.69 s\n",
      "Wall time: 4.75 s\n",
      "mean reward = -678.93600\tthreshold = -14.0\n",
      "CPU times: user 3.63 s, sys: 0 ns, total: 3.63 s\n",
      "Wall time: 3.69 s\n",
      "mean reward = -520.00400\tthreshold = -14.0\n",
      "CPU times: user 4.26 s, sys: 0 ns, total: 4.26 s\n",
      "Wall time: 4.31 s\n",
      "mean reward = -618.78400\tthreshold = -16.5\n",
      "CPU times: user 5.48 s, sys: 4 ms, total: 5.49 s\n",
      "Wall time: 5.54 s\n",
      "mean reward = -792.08400\tthreshold = -17.0\n",
      "CPU times: user 3.2 s, sys: 8 ms, total: 3.21 s\n",
      "Wall time: 3.34 s\n",
      "mean reward = -421.94800\tthreshold = -16.0\n",
      "CPU times: user 3.19 s, sys: 8 ms, total: 3.2 s\n",
      "Wall time: 3.38 s\n",
      "mean reward = -418.26000\tthreshold = -17.5\n",
      "CPU times: user 2.8 s, sys: 4 ms, total: 2.8 s\n",
      "Wall time: 2.86 s\n",
      "mean reward = -370.01600\tthreshold = -15.0\n",
      "CPU times: user 2.84 s, sys: 0 ns, total: 2.84 s\n",
      "Wall time: 2.86 s\n",
      "mean reward = -393.03200\tthreshold = -10.0\n",
      "CPU times: user 4.74 s, sys: 12 ms, total: 4.75 s\n",
      "Wall time: 4.87 s\n",
      "mean reward = -687.89600\tthreshold = -17.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-829eaf014f28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time sessions = [generate_session() for _ in range(n_samples)]#<generate n_samples sessions>]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatch_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jheuristic/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jheuristic/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2080\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-59>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/jheuristic/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jheuristic/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-31edf4ba0fb0>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(t_max)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#pick action from policy (at random with probabilities)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mnew_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice (numpy/random/mtrand/mtrand.c:15566)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/home/jheuristic/anaconda2/lib/python2.7/site-packages/numpy/core/getlimits.pyc\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, dtype)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0m_finfo_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumeric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_samples = 250  #sample this many samples\n",
    "percentile = 50  #take this percent of session with highest rewards\n",
    "smoothing = 0.1  #add this thing to all counts for stability\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    %time sessions = <generate n_samples sessions>]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "    \n",
    "    threshold = <select percentile of your samples> \n",
    "    \n",
    "    elite_states = <select states from sessions where rewards are above threshold> \n",
    "    elite_actions = <select actions from sessions where rewards are above threshold>\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #hint on task above: use np.percentile and numpy-style indexing\n",
    "    \n",
    "    #count actions from elite states\n",
    "    elite_counts = np.zeros_like(policy)+smoothing\n",
    "    \n",
    "    <count all state-action occurences in elite_states and elite_actions>\n",
    "    \n",
    "\n",
    "    policy = <normalize over each state to get probabilities>\n",
    "    \n",
    "    \n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) crossentropy method\n",
    "\n",
    "In this section we will train a neural network policy for continuous action space game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-29 15:35:01,960] Making new env: CartPole-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2759a20590>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEACAYAAACwB81wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZ5l7KktrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f275c7b9c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jheuristic/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#create agent\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "agent = MLPClassifier(hidden_layer_sizes=(20,20),\n",
    "                      activation='tanh',\n",
    "                      warm_start=True, #keep progress between .fit(...) calls\n",
    "                      max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                     )\n",
    "#initialize agent to the dimension of state an amount of actions\n",
    "agent.fit([env.reset()]*n_actions,range(n_actions));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #predict array of action probabilities\n",
    "        probs = agent.predict_proba([s])[0] \n",
    "        \n",
    "        a = <sample action with such probabilities>\n",
    "        \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #record sessions like you did before\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "    return states,actions,total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold =  20.0 \tmean elite reward =  27.2580645161\n",
      "threshold =  19.0 \tmean elite reward =  30.15625\n",
      "threshold =  23.0 \tmean elite reward =  29.2727272727\n",
      "threshold =  26.0 \tmean elite reward =  38.0\n",
      "threshold =  29.0 \tmean elite reward =  42.3333333333\n",
      "threshold =  30.6 \tmean elite reward =  49.4\n",
      "threshold =  40.0 \tmean elite reward =  53.6451612903\n",
      "threshold =  40.6 \tmean elite reward =  61.9666666667\n",
      "threshold =  55.6 \tmean elite reward =  82.6333333333\n",
      "threshold =  54.3 \tmean elite reward =  77.5333333333\n",
      "threshold =  62.0 \tmean elite reward =  86.5625\n",
      "threshold =  68.0 \tmean elite reward =  88.6774193548\n",
      "threshold =  75.0 \tmean elite reward =  105.870967742\n",
      "threshold =  82.3 \tmean elite reward =  107.133333333\n",
      "threshold =  75.6 \tmean elite reward =  113.733333333\n",
      "threshold =  95.3 \tmean elite reward =  135.933333333\n",
      "threshold =  107.3 \tmean elite reward =  138.766666667\n",
      "threshold =  135.9 \tmean elite reward =  180.9\n",
      "threshold =  156.3 \tmean elite reward =  186.966666667\n",
      "threshold =  175.3 \tmean elite reward =  234.5\n",
      "threshold =  198.6 \tmean elite reward =  243.466666667\n",
      "threshold =  223.6 \tmean elite reward =  292.666666667\n",
      "threshold =  266.6 \tmean elite reward =  360.033333333\n",
      "threshold =  257.3 \tmean elite reward =  347.5\n",
      "threshold =  296.3 \tmean elite reward =  368.2\n",
      "threshold =  315.9 \tmean elite reward =  404.6\n",
      "threshold =  332.3 \tmean elite reward =  398.066666667\n",
      "threshold =  327.3 \tmean elite reward =  430.033333333\n",
      "threshold =  376.0 \tmean elite reward =  462.53125\n",
      "threshold =  337.8 \tmean elite reward =  449.133333333\n",
      "threshold =  351.0 \tmean elite reward =  438.064516129\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-3d1b4134fb30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mbatch_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msessions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-59f11b96f87c>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(t_max)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#<sample action with such probabilities>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mnew_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_samples = 100\n",
    "percentile = 70\n",
    "smoothing = 0.01\n",
    "\n",
    "for i in range(100):\n",
    "    #generate new sessions\n",
    "    sessions = [generate_session() for _ in range(n_samples)]\n",
    "\n",
    "    batch_states,batch_actions,batch_rewards = map(np.array,zip(*sessions))\n",
    "    #batch_states: a list of lists of states in each session\n",
    "    #batch_actions: a list of lists of actions in each session\n",
    "    #batch_rewards: a list of floats - total rewards at each session\n",
    "\n",
    "    threshold = <select percentile of your samples>\n",
    "    \n",
    "    elite_states = <select states from sessions where rewards are above threshold>\n",
    "    elite_actions = <select actions from sessions where rewards are above threshold>\n",
    "    \n",
    "    elite_states, elite_actions = map(np.concatenate,[elite_states,elite_actions])\n",
    "    #elite_states: a list of states from top games\n",
    "    #elite_actions: a list of actions from top games\n",
    "    \n",
    "    <fit agent to predict elite_actions(y) from elite_states(X)>\n",
    "\n",
    "\n",
    "    print(\"mean reward = %.5f\\tthreshold = %.1f\"%(np.mean(batch_rewards),threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-01-29 15:42:03,501] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-01-29 15:42:03,505] Clearing 8 monitor files from previous run (because force=True was provided)\n",
      "[2017-01-29 15:42:03,510] Starting new video recorder writing to /home/jheuristic/Documents/wouldbe_rl_course/week1/videos/openaigym.video.1.11230.video000000.mp4\n",
      "[2017-01-29 15:42:09,909] Starting new video recorder writing to /home/jheuristic/Documents/wouldbe_rl_course/week1/videos/openaigym.video.1.11230.video000001.mp4\n",
      "[2017-01-29 15:42:16,704] Starting new video recorder writing to /home/jheuristic/Documents/wouldbe_rl_course/week1/videos/openaigym.video.1.11230.video000008.mp4\n",
      "[2017-01-29 15:42:23,466] Starting new video recorder writing to /home/jheuristic/Documents/wouldbe_rl_course/week1/videos/openaigym.video.1.11230.video000027.mp4\n",
      "[2017-01-29 15:42:31,211] Starting new video recorder writing to /home/jheuristic/Documents/wouldbe_rl_course/week1/videos/openaigym.video.1.11230.video000064.mp4\n",
      "[2017-01-29 15:42:39,019] Finished writing results. You can upload them to the scoreboard via gym.upload('/home/jheuristic/Documents/wouldbe_rl_course/week1/videos')\n"
     ]
    }
   ],
   "source": [
    "#finish recording\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Now try to solve LunarLander-v2 or MountainCar-v0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
